# MoELearn
resource for learning mixture of experts (include paper, notebook, ...)


* Mixture of Experts(MoE) Explained (tensorflow): https://www.kaggle.com/code/newtonbaba12345/mixture-of-experts-moe-explained
*  Mixture of Experts Explained from HuggingFace: https://huggingface.co/blog/moe
*  A Visual Guide to Mixture of Experts (MoE) [Blog]: https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts
*  MoE on CNN: https://github.com/shibuiwilliam/mixture_of_experts_keras
*  ðŸ”¥How does Mixture-of-Experts (MoE) work?ðŸš€: https://www.kaggle.com/code/aliabdin1/how-does-mixture-of-experts-moe-work
*  Mixture-of-Experts (MoE): The Birth and Rise of Conditional Computation: https://cameronrwolfe.substack.com/p/conditional-computation-the-birth

## Youtube
* Introduction to Mixture-of-Experts (MoE): https://youtu.be/kb6eH0zCnl8 
* Playlist: https://www.youtube.com/playlist?list=PLkyja1QUaq_CQ8z97kPvoDbipnIKpBrgM
* Stanford CS25: V1 I Mixture of Experts (MoE) paradigm and the Switch Transformer: https://youtu.be/U8J32Z3qV8s
* Lecture 10.2 â€” Mixtures of Experts â€” [ Deep Learning | Geoffrey Hinton | UofT ]: https://youtu.be/FxrTtRvYQWk
* Multi-Head Mixture-of-Experts: https://youtu.be/8wLG3TIcCXk

## Implement
* Mixture of Experts Architecture Step by Step Explanation and ImplementationðŸ”’ðŸ’»: https://youtu.be/dxh0LjhRHO0
* Mixture of Experts Implementation from scratch: https://youtu.be/aXeU6mVRgiA
* makeMoE: Implement a Sparse Mixture of Experts Language Model from Scratch: https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch
* github implement from lucidrains: https://github.com/lucidrains/mixture-of-experts
