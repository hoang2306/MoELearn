# MoELearn
resource for learning mixture of experts (include paper, notebook, ...)


* Mixture of Experts(MoE) Explained (tensorflow): https://www.kaggle.com/code/newtonbaba12345/mixture-of-experts-moe-explained
*  Mixture of Experts Explained from HuggingFace: https://huggingface.co/blog/moe
*  A Visual Guide to Mixture of Experts (MoE) [Blog]: https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts
*  makeMoE: Implement a Sparse Mixture of Experts Language Model from Scratch: https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch
*  MoE on CNN: https://github.com/shibuiwilliam/mixture_of_experts_keras
*  ðŸ”¥How does Mixture-of-Experts (MoE) work?ðŸš€: https://www.kaggle.com/code/aliabdin1/how-does-mixture-of-experts-moe-work
*  Mixture-of-Experts (MoE): The Birth and Rise of Conditional Computation: https://cameronrwolfe.substack.com/p/conditional-computation-the-birth

## Youtube
* Playlist: https://www.youtube.com/playlist?list=PLkyja1QUaq_CQ8z97kPvoDbipnIKpBrgM

